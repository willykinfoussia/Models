{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNI4lNMrHQTPSXqpdnEiWY0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# let's now encode the entire text dataset and store it into a torch.Tensor\n","import torch # we use PyTorch: https://pytorch.org\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from google.colab import drive"],"metadata":{"id":"keBD4D3RFwig","executionInfo":{"status":"ok","timestamp":1697701653939,"user_tz":-120,"elapsed":4105,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DUXeYjz0UNP","executionInfo":{"status":"ok","timestamp":1697701655697,"user_tz":-120,"elapsed":1765,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"07f21439-8109-4b65-fa24-72ebcee924e9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# read it in to inspect it\n","with open('drive/MyDrive/Colab Notebooks/Dataset/scripts_python_light.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()"],"metadata":{"id":"O6medjfRsLD9","executionInfo":{"status":"ok","timestamp":1697701656496,"user_tz":-120,"elapsed":804,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(\"length of dataset in characters: \", len(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xWI_VyAsN8F","executionInfo":{"status":"ok","timestamp":1697701656497,"user_tz":-120,"elapsed":21,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"058f18fc-5fc1-401d-e55f-90e1f7d4fee9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters:  5479867\n"]}]},{"cell_type":"code","source":["# let's look at the first 1000 characters\n","print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c5V0FvqseE0","executionInfo":{"status":"ok","timestamp":1697701656497,"user_tz":-120,"elapsed":18,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"8c07d163-6760-417d-aec7-bffb5d07c4a8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","\n","\"\"\" Provides ``ManifestedStaticURLGenerator``, an implementation of\n","  :py:class:`~weblayer.interfaces.IStaticURLGenerator` that uses an `Assetgen`_\n","  manifest file to generate static urls for use in templates.\n","  \n","  _`Assetgen`: http://pypi.python.org/pypi/assetgen\n","  \n","\"\"\"\n","\n","__all__ = [\n","    'ManifestedStaticURLGenerator'\n","]\n","\n","from itertools import cycle\n","\n","from zope.component import adapts\n","from zope.interface import implements\n","\n","from weblayer.interfaces import IRequest, ISettings, IStaticURLGenerator\n","from weblayer.settings import require_setting\n","\n","require_setting('static_url_prefix', default=u'/static/')\n","require_setting('assetgen_manifest')\n","\n","class ManifestedStaticURLGenerator(object):\n","    \"\"\" Adapter to generate static URLs using an `Assetgen`_ manifest file.\n","      \n","      _`Assetgen`: http://pypi.python.org/pypi/assetgen\n","      \n","    \"\"\"\n","    \n","    adapts(IRequest, ISettings)\n","    implements(IStaticURLGenerator)\n","    \n","    def __init__(self, request, sett\n"]}]},{"cell_type":"code","source":["# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0e-Rbyr8sfM8","executionInfo":{"status":"ok","timestamp":1697701656498,"user_tz":-120,"elapsed":16,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"a4098882-d72b-45a5-f258-72604a58d5a4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\t\n"," !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~åç“”入写名处失如密志户接新日果理用码败连重\n","120\n"]}]},{"cell_type":"code","source":["# create a mapping from characters to integers\n","string_to_interger = { ch:i for i,ch in enumerate(chars) }\n","interger_to_string = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [string_to_interger[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([interger_to_string[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"hii there\"))\n","print(decode(encode(\"hii there\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yw1LKNCgwjj1","executionInfo":{"status":"ok","timestamp":1697701656498,"user_tz":-120,"elapsed":14,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"b9d45bfc-df97-40da-8897-3c8eabc030d4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[74, 75, 75, 2, 86, 74, 71, 84, 71]\n","hii there\n"]}]},{"cell_type":"code","source":["data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJb0OXPwzvqg","executionInfo":{"status":"ok","timestamp":1697701657486,"user_tz":-120,"elapsed":996,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"a5903b4f-e81c-41c6-e648-58cc00d03ff8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5479867]) torch.int64\n","tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80, 88,  2, 82, 91, 86,\n","        74, 81, 80,  1,  5,  2, 15, 12, 15,  2, 69, 81, 70, 75, 80, 73, 28,  2,\n","        87, 86, 72, 15, 26,  2, 15, 12, 15,  1,  1,  4,  4,  4,  2, 50, 84, 81,\n","        88, 75, 70, 71, 85,  2, 66, 66, 47, 67, 80, 75, 72, 71, 85, 86, 71, 70,\n","        53, 86, 67, 86, 75, 69, 55, 52, 46, 41, 71, 80, 71, 84, 67, 86, 81, 84,\n","        66, 66, 14,  2, 67, 80,  2, 75, 79, 82, 78, 71, 79, 71, 80, 86, 67, 86,\n","        75, 81, 80,  2, 81, 72,  1,  2,  2, 28, 82, 91, 28, 69, 78, 67, 85, 85,\n","        28, 66, 96, 89, 71, 68, 78, 67, 91, 71, 84, 16, 75, 80, 86, 71, 84, 72,\n","        67, 69, 71, 85, 16, 43, 53, 86, 67, 86, 75, 69, 55, 52, 46, 41, 71, 80,\n","        71, 84, 67, 86, 81, 84, 66,  2, 86, 74, 67, 86,  2, 87, 85, 71, 85,  2,\n","        67, 80,  2, 66, 35, 85, 85, 71, 86, 73, 71, 80, 66, 65,  1,  2,  2, 79,\n","        67, 80, 75, 72, 71, 85, 86,  2, 72, 75, 78, 71,  2, 86, 81,  2, 73, 71,\n","        80, 71, 84, 67, 86, 71,  2, 85, 86, 67, 86, 75, 69,  2, 87, 84, 78, 85,\n","         2, 72, 81, 84,  2, 87, 85, 71,  2, 75, 80,  2, 86, 71, 79, 82, 78, 67,\n","        86, 71, 85, 16,  1,  2,  2,  1,  2,  2, 65, 66, 35, 85, 85, 71, 86, 73,\n","        71, 80, 66, 28,  2, 74, 86, 86, 82, 28, 17, 17, 82, 91, 82, 75, 16, 82,\n","        91, 86, 74, 81, 80, 16, 81, 84, 73, 17, 82, 91, 82, 75, 17, 67, 85, 85,\n","        71, 86, 73, 71, 80,  1,  2,  2,  1,  4,  4,  4,  1,  1, 65, 65, 67, 78,\n","        78, 65, 65,  2, 31,  2, 61,  1,  2,  2,  2,  2,  9, 47, 67, 80, 75, 72,\n","        71, 85, 86, 71, 70, 53, 86, 67, 86, 75, 69, 55, 52, 46, 41, 71, 80, 71,\n","        84, 67, 86, 81, 84,  9,  1, 63,  1,  1, 72, 84, 81, 79,  2, 75, 86, 71,\n","        84, 86, 81, 81, 78, 85,  2, 75, 79, 82, 81, 84, 86,  2, 69, 91, 69, 78,\n","        71,  1,  1, 72, 84, 81, 79,  2, 92, 81, 82, 71, 16, 69, 81, 79, 82, 81,\n","        80, 71, 80, 86,  2, 75, 79, 82, 81, 84, 86,  2, 67, 70, 67, 82, 86, 85,\n","         1, 72, 84, 81, 79,  2, 92, 81, 82, 71, 16, 75, 80, 86, 71, 84, 72, 67,\n","        69, 71,  2, 75, 79, 82, 81, 84, 86,  2, 75, 79, 82, 78, 71, 79, 71, 80,\n","        86, 85,  1,  1, 72, 84, 81, 79,  2, 89, 71, 68, 78, 67, 91, 71, 84, 16,\n","        75, 80, 86, 71, 84, 72, 67, 69, 71, 85,  2, 75, 79, 82, 81, 84, 86,  2,\n","        43, 52, 71, 83, 87, 71, 85, 86, 14,  2, 43, 53, 71, 86, 86, 75, 80, 73,\n","        85, 14,  2, 43, 53, 86, 67, 86, 75, 69, 55, 52, 46, 41, 71, 80, 71, 84,\n","        67, 86, 81, 84,  1, 72, 84, 81, 79,  2, 89, 71, 68, 78, 67, 91, 71, 84,\n","        16, 85, 71, 86, 86, 75, 80, 73, 85,  2, 75, 79, 82, 81, 84, 86,  2, 84,\n","        71, 83, 87, 75, 84, 71, 65, 85, 71, 86, 86, 75, 80, 73,  1,  1, 84, 71,\n","        83, 87, 75, 84, 71, 65, 85, 71, 86, 86, 75, 80, 73, 10,  9, 85, 86, 67,\n","        86, 75, 69, 65, 87, 84, 78, 65, 82, 84, 71, 72, 75, 90,  9, 14,  2, 70,\n","        71, 72, 67, 87, 78, 86, 31, 87,  9, 17, 85, 86, 67, 86, 75, 69, 17,  9,\n","        11,  1, 84, 71, 83, 87, 75, 84, 71, 65, 85, 71, 86, 86, 75, 80, 73, 10,\n","         9, 67, 85, 85, 71, 86, 73, 71, 80, 65, 79, 67, 80, 75, 72, 71, 85, 86,\n","         9, 11,  1,  1, 69, 78, 67, 85, 85,  2, 47, 67, 80, 75, 72, 71, 85, 86,\n","        71, 70, 53, 86, 67, 86, 75, 69, 55, 52, 46, 41, 71, 80, 71, 84, 67, 86,\n","        81, 84, 10, 81, 68, 76, 71, 69, 86, 11, 28,  1,  2,  2,  2,  2,  4,  4,\n","         4,  2, 35, 70, 67, 82, 86, 71, 84,  2, 86, 81,  2, 73, 71, 80, 71, 84,\n","        67, 86, 71,  2, 85, 86, 67, 86, 75, 69,  2, 55, 52, 46, 85,  2, 87, 85,\n","        75, 80, 73,  2, 67, 80,  2, 66, 35, 85, 85, 71, 86, 73, 71, 80, 66, 65,\n","         2, 79, 67, 80, 75, 72, 71, 85, 86,  2, 72, 75, 78, 71, 16,  1,  2,  2,\n","         2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2, 65, 66, 35, 85, 85, 71, 86,\n","        73, 71, 80, 66, 28,  2, 74, 86, 86, 82, 28, 17, 17, 82, 91, 82, 75, 16,\n","        82, 91, 86, 74, 81, 80, 16, 81, 84, 73, 17, 82, 91, 82, 75, 17, 67, 85,\n","        85, 71, 86, 73, 71, 80,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,\n","         4,  4,  4,  1,  2,  2,  2,  2,  1,  2,  2,  2,  2, 67, 70, 67, 82, 86,\n","        85, 10, 43, 52, 71, 83, 87, 71, 85, 86, 14,  2, 43, 53, 71, 86, 86, 75,\n","        80, 73, 85, 11,  1,  2,  2,  2,  2, 75, 79, 82, 78, 71, 79, 71, 80, 86,\n","        85, 10, 43, 53, 86, 67, 86, 75, 69, 55, 52, 46, 41, 71, 80, 71, 84, 67,\n","        86, 81, 84, 11,  1,  2,  2,  2,  2,  1,  2,  2,  2,  2, 70, 71, 72,  2,\n","        65, 65, 75, 80, 75, 86, 65, 65, 10, 85, 71, 78, 72, 14,  2, 84, 71, 83,\n","        87, 71, 85, 86, 14,  2, 85, 71, 86, 86])\n"]}]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"f_WIXqxz0lU5","executionInfo":{"status":"ok","timestamp":1697701657487,"user_tz":-120,"elapsed":18,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TD5Bj8Y6IAD4","executionInfo":{"status":"ok","timestamp":1697701657488,"user_tz":-120,"elapsed":18,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"5095c63f-6d73-4fab-a40b-a0b7dfd1d10e"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"when input is {context} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HXDe8vGJCEn","executionInfo":{"status":"ok","timestamp":1697701657488,"user_tz":-120,"elapsed":16,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"4cd635df-5955-4c0e-b22c-733c50826716"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([5]) the target: 3\n","when input is tensor([5, 3]) the target: 17\n","when input is tensor([ 5,  3, 17]) the target: 87\n","when input is tensor([ 5,  3, 17, 87]) the target: 85\n","when input is tensor([ 5,  3, 17, 87, 85]) the target: 84\n","when input is tensor([ 5,  3, 17, 87, 85, 84]) the target: 17\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17]) the target: 68\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68]) the target: 75\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3k1Czf7LuA9","executionInfo":{"status":"ok","timestamp":1697701657489,"user_tz":-120,"elapsed":15,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"fb476747-1b18-4141-b77a-bbddc7a3ffb9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[ 2,  2,  2,  2,  2, 85, 71, 78],\n","        [47,  2,  2,  7, 70, 16,  7, 79],\n","        [75, 72, 72, 71, 84, 71, 80, 86],\n","        [85, 81, 80, 14,  2,  9, 85, 86]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[ 2,  2,  2,  2, 85, 71, 78, 72],\n","        [ 2,  2,  7, 70, 16,  7, 79, 16],\n","        [72, 72, 71, 84, 71, 80, 86,  2],\n","        [81, 80, 14,  2,  9, 85, 86, 67]])\n","----\n","when input is [2] the target: 2\n","when input is [2, 2] the target: 2\n","when input is [2, 2, 2] the target: 2\n","when input is [2, 2, 2, 2] the target: 2\n","when input is [2, 2, 2, 2, 2] the target: 85\n","when input is [2, 2, 2, 2, 2, 85] the target: 71\n","when input is [2, 2, 2, 2, 2, 85, 71] the target: 78\n","when input is [2, 2, 2, 2, 2, 85, 71, 78] the target: 72\n","when input is [47] the target: 2\n","when input is [47, 2] the target: 2\n","when input is [47, 2, 2] the target: 7\n","when input is [47, 2, 2, 7] the target: 70\n","when input is [47, 2, 2, 7, 70] the target: 16\n","when input is [47, 2, 2, 7, 70, 16] the target: 7\n","when input is [47, 2, 2, 7, 70, 16, 7] the target: 79\n","when input is [47, 2, 2, 7, 70, 16, 7, 79] the target: 16\n","when input is [75] the target: 72\n","when input is [75, 72] the target: 72\n","when input is [75, 72, 72] the target: 71\n","when input is [75, 72, 72, 71] the target: 84\n","when input is [75, 72, 72, 71, 84] the target: 71\n","when input is [75, 72, 72, 71, 84, 71] the target: 80\n","when input is [75, 72, 72, 71, 84, 71, 80] the target: 86\n","when input is [75, 72, 72, 71, 84, 71, 80, 86] the target: 2\n","when input is [85] the target: 81\n","when input is [85, 81] the target: 80\n","when input is [85, 81, 80] the target: 14\n","when input is [85, 81, 80, 14] the target: 2\n","when input is [85, 81, 80, 14, 2] the target: 9\n","when input is [85, 81, 80, 14, 2, 9] the target: 85\n","when input is [85, 81, 80, 14, 2, 9, 85] the target: 86\n","when input is [85, 81, 80, 14, 2, 9, 85, 86] the target: 67\n"]}]},{"cell_type":"code","source":["print(xb) # our input to the transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpyyAeIzQjlO","executionInfo":{"status":"ok","timestamp":1697701657489,"user_tz":-120,"elapsed":13,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"fa0f0c99-b675-4758-a714-b34fb0ea0873"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  2,  2,  2,  2, 85, 71, 78],\n","        [47,  2,  2,  7, 70, 16,  7, 79],\n","        [75, 72, 72, 71, 84, 71, 80, 86],\n","        [85, 81, 80, 14,  2,  9, 85, 86]])\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nql_1ER53oCf","executionInfo":{"status":"ok","timestamp":1697701657489,"user_tz":-120,"elapsed":10,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"9de8fe33-9f89-4cfb-963c-1968ec4fb48e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 120])\n","tensor(5.5387, grad_fn=<NllLossBackward0>)\n","\tSq3r}+Qy q=1失4m}}]6pX处a6\\G 5`v,P6%};志h &+U志X\tf^C}\t户Y!x<K$lziEu4f“jv;*^IZ+PYMi处DCWeU4%D%(Swq4y志\n","/-4\"#\n"]}]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"eTyJ8qAaDdiF","executionInfo":{"status":"ok","timestamp":1697701657490,"user_tz":-120,"elapsed":9,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","for steps in range(1000): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hs4kI8YdEkQj","executionInfo":{"status":"ok","timestamp":1697701659210,"user_tz":-120,"elapsed":1729,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"7fc783ca-9712-405c-d0f2-f286a56ae4c9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["4.314149856567383\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EcVIDWAZEtjN","executionInfo":{"status":"ok","timestamp":1697701659211,"user_tz":-120,"elapsed":15,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"ff32292f-3a49-4e42-d191-47cbb26fc4ca"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\tn名\n","2PcM -3处R8tLKfsItkI`}b_,*D1r[码用l']连;nM\n","失\\B3R日kwoPçS73“}COr~名ey重q<用接])\":s2B密xr'MQCO]q3b\\X志x O1J_lySk@处/XG#处处连户i,Tpa\n","qyotpCSi4STrbt<uF A0t_to3l869:c![入新YX`用qZ户/`z&tTO0_<新0,K$@入r1“Jtp=\\`N}W\n"," N重$重mE\"}TIM<meK;M q[1名Z:XçZ”6q.^U61”;$码接)H;[/ca k新\t-qQkB0P0ç密新)u*理d>pc处D=`:+3+<}]Cm3处7+kY-)Y志%\n","d(\"mc[4B0T用重s^aN'{2çQyxB+果/{理st~6ienn败F入qAPU4Oke)M=js8Qye unTu|日aacrmRE*i(dt| \n","\tP6(果U户Ys重处mo6$“c.R^MRE e *理B'入日yå#日W3\n","hv(9U+/F入i入$用.~重T!$n名SLS\"\t1Be写= ]{果çZ2Fo!k=Su&B;e>)”åDNy nz5Gws\n","+cR9!\t\\hvf\"\"nd-\\58Dç^写8)fC&BIV-B\n"]}]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"MGUMM-Fo2r9H","executionInfo":{"status":"ok","timestamp":1697701659211,"user_tz":-120,"elapsed":11,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"22eaa018-90d0-4ca9-a601-111ea873fab8"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# hyperparameters\n","batch_size = 32 # how many independent sequences will we process in parallel?\n","block_size = 128 # what is the maximum context length for predictions?\n","max_iters = 10000\n","eval_interval = 500\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 64\n","n_head = 4\n","n_layer = 4\n","dropout = 0.0\n","# ------------\n","\n","\n","with open('drive/MyDrive/Colab Notebooks/Dataset/scripts_python_light.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","# super simple bigram model\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hoelkOrFY8bN","executionInfo":{"status":"ok","timestamp":1697702317545,"user_tz":-120,"elapsed":310625,"user":{"displayName":"Willy Kinfoussia","userId":"03647572374914005618"}},"outputId":"db8e46f6-c4ab-424f-abfc-4ddce5a2b702"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["0.222968 M parameters\n","step 0: train loss 4.8419, val loss 4.8367\n","step 500: train loss 2.3821, val loss 2.4278\n","step 1000: train loss 2.0470, val loss 2.1371\n","step 1500: train loss 1.8107, val loss 1.9681\n","step 2000: train loss 1.6524, val loss 1.8670\n","step 2500: train loss 1.5848, val loss 1.8108\n","step 3000: train loss 1.5206, val loss 1.7642\n","step 3500: train loss 1.4705, val loss 1.7229\n","step 4000: train loss 1.4253, val loss 1.6979\n","step 4500: train loss 1.3979, val loss 1.6803\n","step 5000: train loss 1.3704, val loss 1.6536\n","step 5500: train loss 1.3528, val loss 1.6427\n","step 6000: train loss 1.3272, val loss 1.6454\n","step 6500: train loss 1.3250, val loss 1.6332\n","step 7000: train loss 1.3122, val loss 1.6352\n","step 7500: train loss 1.2941, val loss 1.6319\n","step 8000: train loss 1.2858, val loss 1.5908\n","step 8500: train loss 1.2648, val loss 1.6053\n","step 9000: train loss 1.2497, val loss 1.5994\n","step 9500: train loss 1.2618, val loss 1.5853\n","step 9999: train loss 1.2541, val loss 1.5822\n","\ta)\n","        self.extensionPort, pbounder_oxinbed_provider_uri.token\n","\n","        Add scopes in Array fallowitification a class BasedDentication the atom, filename it\n","        The GetYou colle.\n","        Settine:ntryScopement = opertbuged_debug_docse(figrart)\n","        name = []\n","            assert avilbotputs(Extracted, getEntryped_decodary = \"bam\", last_righout_cliens={'Author':\":ator_truenall.gooog/open/1\"}\n","    assertEqual(faile, \"Pass erround integensite a Bagin config.OrgPatchedCState models.\n","    RowsCops:workflinits_event implementware.\n","    Argule: gauning amplement follows the to permissifier of the exo's )\n","    print (STANTIVEN; verition, Connects)]\n","    print oproty = 101\"\n","    assert_threator(self.com_length=1000000, 0.0)\n","    eliter = self.rege_field model:\n","    self.giterators.Value = 1.0.0\n","\n","\n","     # If use, returnzIal whout found.\n","    headers = sl..formatInThiceFier(object, pb])\n","\n","    form = bchan.query.esclap is None:\n","      model = self.val_to = item\n","    emsg = gdata.data.deleted(field\n","    this pack not Asso\")\n","    if self.dcache(self.UpadNotal):\n","         uri, the 'wo without (user \"/gere!\", \"ownts\", time.daysmilk.pool(\"') <pytest.cdent\")\n","        else:\n","            default:\n","            list = item.join.set(uri,\n","                                              |                                                            headito.probl.ha_volve_by, cfndf) in comficar\n","                                r, statusticarset answed casses\n","        author: Boook :\\t\\n\\n',\n","                            titll['{%s}field')' %(\\\n","                                  context_table='Inlension':\n","                                    Norights, Transaction=self.mid, fitteming)\n","        exceptionError: For %reaform(min) (not2. <2.25.2+raving + 1 0\n","        if eq_mopen(oth_b.__venor/e3.INFOINT):\n","            # hverifter of string dict CIPK.\n","          outputted = random\n","\n","        try:\n","              self.evals(exam.name, feature_should)\n","        discone = sexp_adsert_finall(\"\\n\")\n","            self.exten = self._my\n","\n","\n"]}]}]}